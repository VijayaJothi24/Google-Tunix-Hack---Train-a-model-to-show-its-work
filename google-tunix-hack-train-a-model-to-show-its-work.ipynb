{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gemma2 2B IT Hackathon Notebook\n\n## ğŸ“Œ Overview\nThis notebook demonstrates fine-tuning and evaluation of the **Gemma2 2B Instruction-Tuned (IT)** model on Kaggle.  \nIt follows the hackathon requirements:\n- Deterministic inference parameters\n- Checkpoint saving during a 9-hour run\n- Single-session evaluation\n- Optional unrestricted mode with Kaggle Model upload\n\n## âš™ï¸ Environment Setup\nInstall required libraries:\n```bash\n%pip install -q transformers torch jax flax orbax-checkpoint kagglehub wandb\n","metadata":{}},{"cell_type":"code","source":"# Standard output tags\nREASONING_START = \"<reasoning>\"\nREASONING_END = \"</reasoning>\"\nSOLUTION_START = \"<answer>\"\nSOLUTION_END = \"</answer>\"\n\n# Deterministic generation parameters\nTEMPERATURE = 1e-4\nTOP_K = 1\nTOP_P = 1.0\nMAX_GENERATION_STEPS = 768\nSEED = 42\n\n# Prompt template\nPROMPT_TEMPLATE = \"your awesome prompt with a placeholder {question}\"\n\n# Paths\nCKPT_DIR = \"/kaggle/working/ckpts\"  # single-session checkpoints (actor/)\nMODEL_ID = \"google/gemma-2-2b-it\"   # base model for single-session mode\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:43:23.334232Z","iopub.execute_input":"2025-12-03T10:43:23.334564Z","iopub.status.idle":"2025-12-03T10:43:23.339478Z","shell.execute_reply.started":"2025-12-03T10:43:23.334539Z","shell.execute_reply":"2025-12-03T10:43:23.338625Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Authentication\nWeights & Biases (W&B): Add your WANDB_API_KEY as a Kaggle Secret or environment variable.\n\npython\nimport os\nos.environ[\"WANDB_API_KEY\"] = \"your_wandb_api_key_here\"\nHugging Face Hub: Request access to Gemma2 2B IT. Add your Hugging Face token:\n\npython\nos.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_hf_token_here\"\nKaggle API (optional for dataset/model upload): Place your kaggle.json in ~/.kaggle/.\n\n# ğŸš€ Usage\nLoad base model and tokenizer\n\npython\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"google/gemma-2-2b-it\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID, token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])\nFine-tuning loop\n\nTrain with LoRA adapters.\n\nSave checkpoints regularly:\n\npython\nsave_actor_checkpoint(step, lora_params)\nEvaluation\n\nLoad the latest checkpoint.\n\nRun deterministic inference with:\n\nCode\nTEMPERATURE=1e-4, TOP_K=1, TOP_P=1.0, MAX_GENERATION_STEPS=768, SEED=42\nUnrestricted Mode (optional)\n\nUpload final Flax-format checkpoints to Kaggle Models.\n\nSet unrestricted_kaggle_model = \"username/model_name\".\n\n# ğŸ“‚ Project Structure\nCode\n/kaggle/working/\n  â”œâ”€â”€ ckpts/actor/<step>/model_params   # Saved checkpoints\n  â”œâ”€â”€ unrestricted/jax/size/...          # For Kaggle Model upload\n\n# ğŸ“ Notes\nEnsure at least one checkpoint is saved during the 9-hour run.\n\nThe last checkpoint will be used for evaluation.\n\nHugging Face access is required for Gemma2 models.\n\n\n# ğŸ™Œ Reflections\nLearned about gated model access and secure token handling.\n\nFaced challenges with JAX/CUDA plugin compatibility.\n\nSuggestions: better Kaggle GPU support for JAX, streamlined Hugging Face gated repo access.","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers==4.44.2 jax==0.4.33 flax==0.8.3 orbax-checkpoint==0.6.3 kagglehub==0.1.6 wandb==0.17.9\n\nimport os, re, random\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nimport flax\nfrom flax import linen as nn\nimport orbax.checkpoint as ocp\nimport wandb\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n# Determinism\nrandom.seed(SEED)\nnp.random.seed(SEED)\nset_seed(SEED)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:28:20.794572Z","iopub.execute_input":"2025-12-03T10:28:20.794954Z","iopub.status.idle":"2025-12-03T10:29:32.494895Z","shell.execute_reply.started":"2025-12-03T10:28:20.794928Z","shell.execute_reply":"2025-12-03T10:29:32.494033Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m685.9/685.9 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m269.7/269.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.1/85.1 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-12-03 10:29:21.127890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764757761.356237      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764757761.425645      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\n\n# Option 1: set environment variable directly\nos.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"\"\n\n# Option 2: use huggingface_hub login\nlogin(token=\"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:49:18.971974Z","iopub.execute_input":"2025-12-03T10:49:18.972283Z","iopub.status.idle":"2025-12-03T10:49:19.018129Z","shell.execute_reply.started":"2025-12-03T10:49:18.972263Z","shell.execute_reply":"2025-12-03T10:49:19.017308Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"google/gemma-2-2b-it\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"],\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:49:21.883029Z","iopub.execute_input":"2025-12-03T10:49:21.883332Z","iopub.status.idle":"2025-12-03T10:49:44.543618Z","shell.execute_reply.started":"2025-12-03T10:49:21.883312Z","shell.execute_reply":"2025-12-03T10:49:44.542749Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc0f097fd3fd465f8fdca1679585cbc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd9f364412fc45dd9c5ff437bbe09d15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"937398ba76d74f2a85acaf2540823f01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c54baf17ea63418bb55756c5fa84f146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f26202d6dc64b379911feb224339671"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b4a034e86444f0e8b637c56ef667c9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd77a227696d4663ab4c14853614e9c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"443d16c63afe45c2a40cc06e9d42aebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e82474d58f614f258927c9551d262031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b1eba15a8c24bc58b52c1288d638a9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bfe7c4b831643038b22d8dee569ed1e"}},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# Load base model for reference/inference (PyTorch)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:50:00.284294Z","iopub.execute_input":"2025-12-03T10:50:00.287146Z","iopub.status.idle":"2025-12-03T10:50:31.974420Z","shell.execute_reply.started":"2025-12-03T10:50:00.287036Z","shell.execute_reply":"2025-12-03T10:50:31.971937Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a62d3086a494139a114a709be11ece2"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"import os\n\n# Option 1: set directly in code (not recommended for sharing notebooks)\nos.environ[\"WANDB_API_KEY\"] = \"\"\n\n# Option 2: safer â€” use Kaggle Secrets (preferred)\n# In Kaggle: Notebook â†’ Add-ons â†’ Secrets â†’ Add a new secret with key \"WANDB_API_KEY\"\n# Then access it like:\nwandb_api_key = os.environ.get(\"WANDB_API_KEY\")\nprint(\"W&B key loaded:\", bool(wandb_api_key))  # True if available\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:50:31.978365Z","iopub.execute_input":"2025-12-03T10:50:31.978888Z","iopub.status.idle":"2025-12-03T10:50:31.984651Z","shell.execute_reply.started":"2025-12-03T10:50:31.978858Z","shell.execute_reply":"2025-12-03T10:50:31.983773Z"}},"outputs":[{"name":"stdout","text":"W&B key loaded: True\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Standard output tags\nREASONING_START = \"<reasoning>\"\nREASONING_END = \"</reasoning>\"\nSOLUTION_START = \"<answer>\"\nSOLUTION_END = \"</answer>\"\n\n# Deterministic generation parameters\nTEMPERATURE = 1e-4\nTOP_K = 1\nTOP_P = 1.0\nMAX_GENERATION_STEPS = 768\nSEED = 42\n\n# Prompt template\nPROMPT_TEMPLATE = \"your awesome prompt with a placeholder {question}\"\n\n# Paths\nCKPT_DIR = \"/kaggle/working/ckpts\"  # single-session checkpoints (actor/)\nMODEL_ID = \"google/gemma-2-2b-it\"   # base model for single-session mode\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:50:54.009815Z","iopub.execute_input":"2025-12-03T10:50:54.010114Z","iopub.status.idle":"2025-12-03T10:50:54.015217Z","shell.execute_reply.started":"2025-12-03T10:50:54.010093Z","shell.execute_reply":"2025-12-03T10:50:54.014332Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"!pip install -q transformers==4.44.2 jax==0.4.33 flax==0.8.3 orbax-checkpoint==0.6.3 kagglehub==0.1.6 wandb==0.17.9\n\nimport os, re, random\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nimport flax\nfrom flax import linen as nn\nimport orbax.checkpoint as ocp\nimport wandb\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n# Determinism\nrandom.seed(SEED)\nnp.random.seed(SEED)\nset_seed(SEED)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:51:06.938433Z","iopub.execute_input":"2025-12-03T10:51:06.939138Z","iopub.status.idle":"2025-12-03T10:51:13.748641Z","shell.execute_reply.started":"2025-12-03T10:51:06.939109Z","shell.execute_reply":"2025-12-03T10:51:13.747484Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# Load base model for reference/inference (PyTorch)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:51:21.404037Z","iopub.execute_input":"2025-12-03T10:51:21.405036Z","iopub.status.idle":"2025-12-03T10:51:34.245550Z","shell.execute_reply.started":"2025-12-03T10:51:21.404991Z","shell.execute_reply":"2025-12-03T10:51:34.244728Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ff9aec35c7438d98d3d09697dcba13"}},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"%pip install --upgrade jax jaxlib==0.4.33+cuda12.cudnn89 \\\n  -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:53:44.834465Z","iopub.execute_input":"2025-12-03T10:53:44.835430Z","iopub.status.idle":"2025-12-03T10:53:47.671246Z","shell.execute_reply.started":"2025-12-03T10:53:44.835398Z","shell.execute_reply":"2025-12-03T10:53:47.669760Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nRequirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (0.4.33)\nCollecting jax\n  Downloading jax-0.8.1-py3-none-any.whl.metadata (13 kB)\n\u001b[31mERROR: Ignored the following yanked versions: 0.4.32\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement jaxlib==0.4.33+cuda12.cudnn89 (from versions: 0.3.24+cuda11.cudnn805, 0.3.24+cuda11.cudnn82, 0.3.25+cuda11.cudnn805, 0.3.25+cuda11.cudnn82, 0.4.1+cuda11.cudnn82, 0.4.1+cuda11.cudnn86, 0.4.2+cuda11.cudnn82, 0.4.2+cuda11.cudnn86, 0.4.3+cuda11.cudnn82, 0.4.3+cuda11.cudnn86, 0.4.4+cuda11.cudnn82, 0.4.4+cuda11.cudnn86, 0.4.6, 0.4.6+cuda11.cudnn82, 0.4.6+cuda11.cudnn86, 0.4.7, 0.4.7+cuda11.cudnn82, 0.4.7+cuda11.cudnn86, 0.4.7+cuda12.cudnn88, 0.4.9, 0.4.9+cuda11.cudnn86, 0.4.9+cuda12.cudnn88, 0.4.10, 0.4.10+cuda11.cudnn86, 0.4.10+cuda12.cudnn88, 0.4.11, 0.4.11+cuda11.cudnn86, 0.4.11+cuda12.cudnn88, 0.4.12, 0.4.12+cuda11.cudnn86, 0.4.12+cuda12.cudnn89, 0.4.13, 0.4.13+cuda11.cudnn86, 0.4.13+cuda12.cudnn89, 0.4.14, 0.4.14+cuda11.cudnn86, 0.4.14+cuda12.cudnn89, 0.4.16, 0.4.16+cuda11.cudnn86, 0.4.16+cuda12.cudnn89, 0.4.17, 0.4.17+cuda11.cudnn86, 0.4.17+cuda12.cudnn89, 0.4.18, 0.4.18+cuda11.cudnn86, 0.4.18+cuda12.cudnn89, 0.4.19, 0.4.19+cuda11.cudnn86, 0.4.19+cuda12.cudnn89, 0.4.20, 0.4.20+cuda11.cudnn86, 0.4.20+cuda12.cudnn89, 0.4.21, 0.4.21+cuda11.cudnn86, 0.4.21+cuda12.cudnn89, 0.4.22, 0.4.22+cuda11.cudnn86, 0.4.22+cuda12.cudnn89, 0.4.23, 0.4.23+cuda11.cudnn86, 0.4.23+cuda12.cudnn89, 0.4.24, 0.4.24+cuda11.cudnn86, 0.4.24+cuda12.cudnn89, 0.4.25, 0.4.25+cuda11.cudnn86, 0.4.25+cuda12.cudnn89, 0.4.26, 0.4.26+cuda12.cudnn89, 0.4.27, 0.4.27+cuda12.cudnn89, 0.4.28, 0.4.28+cuda12.cudnn89, 0.4.29, 0.4.29+cuda12.cudnn91, 0.4.30, 0.4.31, 0.4.33, 0.4.34, 0.4.35, 0.4.36, 0.4.38, 0.5.0, 0.5.1, 0.5.3, 0.6.0, 0.6.1, 0.6.2, 0.7.0, 0.7.1, 0.7.2, 0.8.0, 0.8.1)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for jaxlib==0.4.33+cuda12.cudnn89\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Placeholder Flax module to host LoRA params\nclass LoRALayer(nn.Module):\n    rank: int\n    def setup(self):\n        self.alpha = self.param(\"alpha\", nn.initializers.ones, (1,))\n        # Example LoRA params\n        self.lora_w = self.param(\"lora_w\", nn.initializers.normal(stddev=0.02), (self.rank,))\n\n    def __call__(self, x):\n        return x  # Integrate with your transformer blocks in real training\n\nclass LoRAPolicy(nn.Module):\n    rank: int = 8\n    def setup(self):\n        self.layer = LoRALayer(rank=self.rank)\n    def __call__(self, x):\n        return self.layer(x)\n\n# Initialize policy state\nrng = jax.random.PRNGKey(SEED)\nlora_policy = LoRAPolicy(rank=8)\nparams = lora_policy.init(rng, jnp.zeros((1,)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:54:52.388390Z","iopub.execute_input":"2025-12-03T10:54:52.388751Z","iopub.status.idle":"2025-12-03T10:54:52.407639Z","shell.execute_reply.started":"2025-12-03T10:54:52.388724Z","shell.execute_reply":"2025-12-03T10:54:52.406179Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# W&B (workaround for logging in eval stage)\nwandb.init(project=\"tunix-train\", mode=\"disabled\")\n\n# Ensure checkpoint dirs\nactor_dir = os.path.join(CKPT_DIR, \"actor\")\nos.makedirs(actor_dir, exist_ok=True)\n\n# Orbax checkpointer\ncheckpointer = ocp.StandardCheckpointer()\n\ndef save_actor_checkpoint(step: int, lora_params):\n    step_dir = os.path.join(actor_dir, str(step))\n    os.makedirs(step_dir, exist_ok=True)\n    target = jax.tree.map(lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype), lora_params)\n    save_path = os.path.join(step_dir, \"model_params\")\n    checkpointer.save(save_path, lora_params, force=True)\n\n# Your awesome finetuning code\n# Example loop to ensure at least one checkpoint within 9hr:\ntotal_steps = 10  # adjust to your training plan\nlora_params = params  # replace with updated params during training\n\nfor step in range(1, total_steps + 1):\n    # ... perform training step here ...\n    # lora_params = updated params from training\n    if step % 5 == 0 or step == total_steps:\n        save_actor_checkpoint(step, lora_params)\n\nprint(\"Training complete. Last checkpoint saved at step:\", total_steps)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:55:35.251634Z","iopub.execute_input":"2025-12-03T10:55:35.252033Z","iopub.status.idle":"2025-12-03T10:55:43.267433Z","shell.execute_reply.started":"2025-12-03T10:55:35.252002Z","shell.execute_reply":"2025-12-03T10:55:43.266239Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Disabling the wandb service is deprecated as of version 0.18.0 and will be removed in version 0.19.0.\n","output_type":"stream"},{"name":"stdout","text":"Training complete. Last checkpoint saved at step: 10\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Load latest checkpoint\nactor_ckpt_dir = os.path.join(CKPT_DIR, \"actor\")\nlatest_step = -1\nif os.path.exists(actor_ckpt_dir):\n    for item in os.listdir(actor_ckpt_dir):\n        if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r\"^\\d+$\", item):\n            step = int(item)\n            if step > latest_step:\n                latest_step = step\n\nif latest_step == -1:\n    raise FileNotFoundError(f\"No checkpoints found in {actor_ckpt_dir}\")\n\nprint(f\"Latest checkpoint step: {latest_step}\")\n\nwandb.init(project='tunix-eval', mode=\"disabled\")  # logging bug workaround\n\ntrained_ckpt_path = os.path.join(CKPT_DIR, \"actor\", str(latest_step), \"model_params\")\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    params\n)\ntrained_lora_params = ocp.StandardCheckpointer().restore(trained_ckpt_path, target=abs_params)\n\n# Update policy with trained LoRA params\ndef tree_update(tree, new_tree):\n    return jax.tree.map(lambda a, b: b, tree, new_tree)\n\nparams = tree_update(params, trained_lora_params)\n\n# Simple sampler wrapper (placeholder)\nclass Sampler:\n    def __init__(self, transformer, tokenizer):\n        self.transformer = transformer\n        self.tokenizer = tokenizer\n    def generate(self, question):\n        prompt = PROMPT_TEMPLATE.format(question=question)\n        # Use base_model for text generation; LoRA would apply in a full Flax transformer setup\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n        outputs = base_model.generate(\n            **inputs,\n            max_new_tokens=128,\n            temperature=TEMPERATURE,\n            top_k=TOP_K,\n            top_p=TOP_P,\n            do_sample=(TEMPERATURE > 0.0)\n        )\n        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nsampler = Sampler(lora_policy, tokenizer)\n\n# AI-based evaluation scaffold\nclass TunixHackathonEval:\n    questions = [\"What is LoRA?\", \"Explain Bayesian priors.\", \"Trade-offs in RLHF.\"]\n    ai_judge = \"ai\"\n\n    def __init__(self, sampler, prompt_template, temperature, top_k, top_p, seed):\n        self.sampler = sampler\n        self.template = prompt_template\n        self.temperature = temperature\n        self.top_k = top_k\n        self.top_p = top_p\n        self.seed = seed\n\n    def evaluate(self):\n        results = []\n        for q in self.questions:\n            ans = self.sampler.generate(q)\n            results.append({\"question\": q, \"answer\": ans})\n        return results\n\nPROMPT = PROMPT_TEMPLATE\nResult = TunixHackathonEval(sampler, PROMPT, TEMPERATURE, TOP_K, TOP_P, SEED).evaluate()\n\nprint(REASONING_START + \"eval_complete\" + REASONING_END)\nprint(SOLUTION_START + str(Result) + SOLUTION_END)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:55:58.225689Z","iopub.execute_input":"2025-12-03T10:55:58.226533Z","iopub.status.idle":"2025-12-03T10:58:47.392790Z","shell.execute_reply.started":"2025-12-03T10:55:58.226503Z","shell.execute_reply":"2025-12-03T10:58:47.391703Z"}},"outputs":[{"name":"stdout","text":"Latest checkpoint step: 10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/orbax/checkpoint/type_handlers.py:1493: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"<reasoning>eval_complete</reasoning>\n<answer>[{'question': 'What is LoRA?', 'answer': \"your awesome prompt with a placeholder What is LoRA?\\n\\n**What is LoRA?**\\n\\nLoRA, or Low-Rank Adaptation, is a powerful technique for fine-tuning large language models (LLMs) efficiently. It allows you to adapt a pre-trained LLM to a specific task or domain without needing to retrain the entire model. \\n\\n**How LoRA Works:**\\n\\n1. **Freezing the Pre-trained Model:** The original pre-trained LLM's weights are frozen, meaning they are not updated during the fine-tuning process. This prevents catastrophic forgetting and ensures the model retains its general knowledge.\\n\\n2. **Introducing Low-Rank Matrices:**\"}, {'question': 'Explain Bayesian priors.', 'answer': \"your awesome prompt with a placeholder Explain Bayesian priors.\\n\\n**Bayesian priors** are a way of incorporating prior knowledge or beliefs about a parameter into a Bayesian analysis. They are essentially **probability distributions** that represent our initial assumptions about the parameter before we observe any data. \\n\\n**Here's a breakdown:**\\n\\n* **Prior Distribution:** This is the probability distribution that reflects our initial beliefs about the parameter. It can be based on previous research, expert opinions, or even just our intuition.\\n* **Likelihood Function:** This function describes the probability of observing the data given a specific value of the parameter.\\n* **Posterior Distribution:** This is the updated probability distribution after incorporating the data\"}, {'question': 'Trade-offs in RLHF.', 'answer': 'your awesome prompt with a placeholder Trade-offs in RLHF.\\n\\n## Trade-offs in RLHF\\n\\n**Prompt:**\\n\\nYou are a helpful and informative AI assistant. Explain the trade-offs involved in using Reinforcement Learning from Human Feedback (RLHF) for training large language models. \\n\\n**Response:**\\n\\nReinforcement Learning from Human Feedback (RLHF) is a powerful technique for training large language models (LLMs) to be more aligned with human preferences. However, it comes with several trade-offs that need to be carefully considered. \\n\\n**Here are some key trade-offs:**\\n\\n**1. Cost and Efficiency:**\\n\\n* **High Cost:** RLHF is computationally expensive'}]</answer>\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Publish Flax-format files to Kaggle Models (unrestricted mode)\n# Ensure you have converted/packaged your Flax parameters and configs accordingly.\n\nimport kagglehub\n\n# Example model path layout:\n# /kaggle/working/unrestricted/jax/size/actor/<latest_step>/model_params\nunrestricted_root = \"/kaggle/working/unrestricted/jax/size\"\nos.makedirs(unrestricted_root, exist_ok=True)\n\n# Copy/prepare your final checkpoint to unrestricted_root as required by Tunix\n# (You may need to mirror the directory structure used in evaluation.)\nfinal_src = trained_ckpt_path  # from above\nfinal_dst = os.path.join(unrestricted_root, \"actor\", str(latest_step), \"model_params\")\nos.makedirs(os.path.dirname(final_dst), exist_ok=True)\n\n# Orbax checkpoints are directories; ensure proper copying if needed.\n# For simplicity, re-save into final_dst\nocp.StandardCheckpointer().save(final_dst, trained_lora_params, force=True)\n\n# Upload to Kaggle Models\n# Replace 'your_username/model_name' and ensure Visibility set to Public in model settings.\n# Note: In practice, you may need to use Kaggle web UI to publish as a Model with JAX/Flax files.\n# kagglehub.model_upload(model_id=\"your_username/model_name\", src_dir=\"/kaggle/working/unrestricted\")\n\n# Record your unrestricted model ID here (make public and ensure loadable):\nunrestricted_kaggle_model = \"your_username/model_name\"\nprint(\"Unrestricted mode model ID:\", unrestricted_kaggle_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T10:58:47.508618Z","iopub.execute_input":"2025-12-03T10:58:47.509043Z","iopub.status.idle":"2025-12-03T10:58:47.527996Z","shell.execute_reply.started":"2025-12-03T10:58:47.509013Z","shell.execute_reply":"2025-12-03T10:58:47.527045Z"}},"outputs":[{"name":"stdout","text":"Unrestricted mode model ID: your_username/model_name\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Load uploaded checkpoint for unrestricted eval\n# May require appending exact subpaths depending on your publication layout\n# Example (adjust if needed):\n# trained_ckpt_path = kagglehub.model_download(unrestricted_kaggle_model + \"/jax/size\")\n\n# If your structure includes actor/<step>/model_params:\n# trained_ckpt_path = kagglehub.model_download(unrestricted_kaggle_model + \"/jax/size/actor/\" + str(latest_step) + \"/model_params\")\n\n# Restore and update params (same as single-session)\nabs_params = jax.tree.map(lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype), params)\ntrained_lora_params = ocp.StandardCheckpointer().restore(trained_ckpt_path, target=abs_params)\nparams = tree_update(params, trained_lora_params)\n\n# Reuse TunixHackathonEval from above\nResult = TunixHackathonEval(sampler, PROMPT, TEMPERATURE, TOP_K, TOP_P, SEED).evaluate()\nprint(REASONING_START + \"unrestricted_eval_complete\" + REASONING_END)\nprint(SOLUTION_START + str(Result) + SOLUTION_END)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:32.834794Z","iopub.execute_input":"2025-12-03T11:09:32.835257Z","iopub.status.idle":"2025-12-03T11:12:14.088578Z","shell.execute_reply.started":"2025-12-03T11:09:32.835226Z","shell.execute_reply":"2025-12-03T11:12:14.087552Z"}},"outputs":[{"name":"stdout","text":"<reasoning>unrestricted_eval_complete</reasoning>\n<answer>[{'question': 'What is LoRA?', 'answer': \"your awesome prompt with a placeholder What is LoRA?\\n\\n**What is LoRA?**\\n\\nLoRA, or Low-Rank Adaptation, is a powerful technique for fine-tuning large language models (LLMs) efficiently. It allows you to adapt a pre-trained LLM to a specific task or domain without needing to retrain the entire model. \\n\\n**How LoRA Works:**\\n\\n1. **Freezing the Pre-trained Model:** The original pre-trained LLM's weights are frozen, meaning they are not updated during the fine-tuning process. This prevents catastrophic forgetting and ensures the model retains its general knowledge.\\n\\n2. **Introducing Low-Rank Matrices:**\"}, {'question': 'Explain Bayesian priors.', 'answer': \"your awesome prompt with a placeholder Explain Bayesian priors.\\n\\n**Bayesian priors** are a way of incorporating prior knowledge or beliefs about a parameter into a Bayesian analysis. They are essentially **probability distributions** that represent our initial assumptions about the parameter before we observe any data. \\n\\n**Here's a breakdown:**\\n\\n* **Prior Distribution:** This is the probability distribution that reflects our initial beliefs about the parameter. It can be based on previous research, expert opinions, or even just our intuition.\\n* **Likelihood Function:** This function describes the probability of observing the data given a specific value of the parameter.\\n* **Posterior Distribution:** This is the updated probability distribution after incorporating the data\"}, {'question': 'Trade-offs in RLHF.', 'answer': 'your awesome prompt with a placeholder Trade-offs in RLHF.\\n\\n## Trade-offs in RLHF\\n\\n**Prompt:**\\n\\nYou are a helpful and informative AI assistant. Explain the trade-offs involved in using Reinforcement Learning from Human Feedback (RLHF) for training large language models. \\n\\n**Response:**\\n\\nReinforcement Learning from Human Feedback (RLHF) is a powerful technique for training large language models (LLMs) to be more aligned with human preferences. However, it comes with several trade-offs that need to be carefully considered. \\n\\n**Here are some key trade-offs:**\\n\\n**1. Cost and Efficiency:**\\n\\n* **High Cost:** RLHF is computationally expensive'}]</answer>\n","output_type":"stream"}],"execution_count":48}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gemma2 2B IT Hackathon Notebook\n\n## üìå Overview\nThis notebook demonstrates fine-tuning and evaluation of the **Gemma2 2B Instruction-Tuned (IT)** model on Kaggle.  \nIt follows the hackathon requirements:\n- Deterministic inference parameters\n- Checkpoint saving during a 9-hour run\n- Single-session evaluation\n- Optional unrestricted mode with Kaggle Model upload\n\n## ‚öôÔ∏è Environment Setup\nInstall required libraries:\n```bash\n%pip install -q transformers torch jax flax orbax-checkpoint kagglehub wandb\n","metadata":{}},{"cell_type":"code","source":"# Standard output tags\nREASONING_START = \"<reasoning>\"\nREASONING_END = \"</reasoning>\"\nSOLUTION_START = \"<answer>\"\nSOLUTION_END = \"</answer>\"\n\n# Deterministic generation parameters\nTEMPERATURE = 1e-4\nTOP_K = 1\nTOP_P = 1.0\nMAX_GENERATION_STEPS = 768\nSEED = 42\n\n# Prompt template\nPROMPT_TEMPLATE = \"your awesome prompt with a placeholder {question}\"\n\n# Paths\nCKPT_DIR = \"/kaggle/working/ckpts\"  # single-session checkpoints (actor/)\nMODEL_ID = \"google/gemma-2-2b-it\"   # base model for single-session mode\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:14:09.539569Z","iopub.execute_input":"2025-12-30T18:14:09.540643Z","iopub.status.idle":"2025-12-30T18:14:09.545579Z","shell.execute_reply.started":"2025-12-30T18:14:09.540610Z","shell.execute_reply":"2025-12-30T18:14:09.544618Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Authentication\nWeights & Biases (W&B): Add your WANDB_API_KEY as a Kaggle Secret or environment variable.\n\npython\nimport os\nos.environ[\"WANDB_API_KEY\"] = \"your_wandb_api_key_here\"\nHugging Face Hub: Request access to Gemma2 2B IT. Add your Hugging Face token:\n\npython\nos.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_hf_token_here\"\nKaggle API (optional for dataset/model upload): Place your kaggle.json in ~/.kaggle/.\n\n# üöÄ Usage\nLoad base model and tokenizer\n\npython\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"google/gemma-2-2b-it\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID, token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])\nFine-tuning loop\n\nTrain with LoRA adapters.\n\nSave checkpoints regularly:\n\npython\nsave_actor_checkpoint(step, lora_params)\nEvaluation\n\nLoad the latest checkpoint.\n\nRun deterministic inference with:\n\nCode\nTEMPERATURE=1e-4, TOP_K=1, TOP_P=1.0, MAX_GENERATION_STEPS=768, SEED=42\nUnrestricted Mode (optional)\n\nUpload final Flax-format checkpoints to Kaggle Models.\n\nSet unrestricted_kaggle_model = \"username/model_name\".\n\n# üìÇ Project Structure\nCode\n/kaggle/working/\n  ‚îú‚îÄ‚îÄ ckpts/actor/<step>/model_params   # Saved checkpoints\n  ‚îú‚îÄ‚îÄ unrestricted/jax/size/...          # For Kaggle Model upload\n\n# üìù Notes\nEnsure at least one checkpoint is saved during the 9-hour run.\n\nThe last checkpoint will be used for evaluation.\n\nHugging Face access is required for Gemma2 models.\n\n\n# üôå Reflections\nLearned about gated model access and secure token handling.\n\nFaced challenges with JAX/CUDA plugin compatibility.\n\nSuggestions: better Kaggle GPU support for JAX, streamlined Hugging Face gated repo access.","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers==4.44.2 jax==0.4.33 flax==0.8.3 orbax-checkpoint==0.6.3 kagglehub==0.1.6 wandb==0.17.9\n\nimport os, re, random\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nimport flax\nfrom flax import linen as nn\nimport orbax.checkpoint as ocp\nimport wandb\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n# Determinism\nrandom.seed(SEED)\nnp.random.seed(SEED)\nset_seed(SEED)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:14:15.015052Z","iopub.execute_input":"2025-12-30T18:14:15.015373Z","iopub.status.idle":"2025-12-30T18:15:38.628418Z","shell.execute_reply.started":"2025-12-30T18:14:15.015346Z","shell.execute_reply":"2025-12-30T18:15:38.627277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\n\n# Option 1: set environment variable directly\nos.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"\"\n\n# Option 2: use huggingface_hub login\nlogin(token=\"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:25:42.217319Z","iopub.execute_input":"2025-12-30T18:25:42.217815Z","iopub.status.idle":"2025-12-30T18:25:42.263927Z","shell.execute_reply.started":"2025-12-30T18:25:42.217781Z","shell.execute_reply":"2025-12-30T18:25:42.262823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"google/gemma-2-2b-it\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"],\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:25:44.655056Z","iopub.execute_input":"2025-12-30T18:25:44.655382Z","iopub.status.idle":"2025-12-30T18:26:11.262507Z","shell.execute_reply.started":"2025-12-30T18:25:44.655356Z","shell.execute_reply":"2025-12-30T18:26:11.261528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# Load base model for reference/inference (PyTorch)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:27:10.320384Z","iopub.execute_input":"2025-12-30T18:27:10.320728Z","iopub.status.idle":"2025-12-30T18:27:18.239183Z","shell.execute_reply.started":"2025-12-30T18:27:10.320702Z","shell.execute_reply":"2025-12-30T18:27:18.237927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Option 1: set directly in code (not recommended for sharing notebooks)\nos.environ[\"WANDB_API_KEY\"] = \"\"\n\n# Option 2: safer ‚Äî use Kaggle Secrets (preferred)\n# In Kaggle: Notebook ‚Üí Add-ons ‚Üí Secrets ‚Üí Add a new secret with key \"WANDB_API_KEY\"\n# Then access it like:\nwandb_api_key = os.environ.get(\")\nprint(\"W&B key loaded:\", bool(wandb_api_key))  # True if available\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:16:09.126783Z","iopub.execute_input":"2025-12-30T19:16:09.127071Z","iopub.status.idle":"2025-12-30T19:16:09.135615Z","shell.execute_reply.started":"2025-12-30T19:16:09.127046Z","shell.execute_reply":"2025-12-30T19:16:09.134692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standard output tags\nREASONING_START = \"<reasoning>\"\nREASONING_END = \"</reasoning>\"\nSOLUTION_START = \"<answer>\"\nSOLUTION_END = \"</answer>\"\n\n# Deterministic generation parameters\nTEMPERATURE = 1e-4\nTOP_K = 1\nTOP_P = 1.0\nMAX_GENERATION_STEPS = 768\nSEED = 42\n\n# Prompt template\nPROMPT_TEMPLATE = \"your awesome prompt with a placeholder {question}\"\n\n# Paths\nCKPT_DIR = \"/kaggle/working/ckpts\"  # single-session checkpoints (actor/)\nMODEL_ID = \"google/gemma-2-2b-it\"   # base model for single-session mode\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:16:09.136778Z","iopub.execute_input":"2025-12-30T19:16:09.137133Z","iopub.status.idle":"2025-12-30T19:16:09.154757Z","shell.execute_reply.started":"2025-12-30T19:16:09.137104Z","shell.execute_reply":"2025-12-30T19:16:09.153570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers==4.44.2 jax==0.4.33 flax==0.8.3 orbax-checkpoint==0.6.3 kagglehub==0.1.6 wandb==0.17.9\n\nimport os, re, random\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nimport flax\nfrom flax import linen as nn\nimport orbax.checkpoint as ocp\nimport wandb\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n\n# Determinism\nrandom.seed(SEED)\nnp.random.seed(SEED)\nset_seed(SEED)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:16:09.155968Z","iopub.execute_input":"2025-12-30T19:16:09.156400Z","iopub.status.idle":"2025-12-30T19:16:14.048963Z","shell.execute_reply.started":"2025-12-30T19:16:09.156335Z","shell.execute_reply":"2025-12-30T19:16:14.047614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# Load base model for reference/inference (PyTorch)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:27:38.702161Z","iopub.execute_input":"2025-12-30T18:27:38.702574Z","iopub.status.idle":"2025-12-30T18:27:44.874852Z","shell.execute_reply.started":"2025-12-30T18:27:38.702538Z","shell.execute_reply":"2025-12-30T18:27:44.873687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install --upgrade jax jaxlib==0.4.33+cuda12.cudnn89 \\\n  -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:27:44.876729Z","iopub.execute_input":"2025-12-30T18:27:44.877031Z","iopub.status.idle":"2025-12-30T18:27:47.695899Z","shell.execute_reply.started":"2025-12-30T18:27:44.877007Z","shell.execute_reply":"2025-12-30T18:27:47.694690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Placeholder Flax module to host LoRA params\nclass LoRALayer(nn.Module):\n    rank: int\n    def setup(self):\n        self.alpha = self.param(\"alpha\", nn.initializers.ones, (1,))\n        # Example LoRA params\n        self.lora_w = self.param(\"lora_w\", nn.initializers.normal(stddev=0.02), (self.rank,))\n\n    def __call__(self, x):\n        return x  # Integrate with your transformer blocks in real training\n\nclass LoRAPolicy(nn.Module):\n    rank: int = 8\n    def setup(self):\n        self.layer = LoRALayer(rank=self.rank)\n    def __call__(self, x):\n        return self.layer(x)\n\n# Initialize policy state\nrng = jax.random.PRNGKey(SEED)\nlora_policy = LoRAPolicy(rank=8)\nparams = lora_policy.init(rng, jnp.zeros((1,)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:16:14.050492Z","iopub.execute_input":"2025-12-30T19:16:14.050894Z","iopub.status.idle":"2025-12-30T19:16:14.073623Z","shell.execute_reply.started":"2025-12-30T19:16:14.050857Z","shell.execute_reply":"2025-12-30T19:16:14.072131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# W&B (workaround for logging in eval stage)\nwandb.init(project=\"tunix-train\", mode=\"disabled\")\n\n# Ensure checkpoint dirs\nactor_dir = os.path.join(CKPT_DIR, \"actor\")\nos.makedirs(actor_dir, exist_ok=True)\n\n# Orbax checkpointer\ncheckpointer = ocp.StandardCheckpointer()\n\ndef save_actor_checkpoint(step: int, lora_params):\n    step_dir = os.path.join(actor_dir, str(step))\n    os.makedirs(step_dir, exist_ok=True)\n    target = jax.tree.map(lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype), lora_params)\n    save_path = os.path.join(step_dir, \"model_params\")\n    checkpointer.save(save_path, lora_params, force=True)\n\n# Your awesome finetuning code\n# Example loop to ensure at least one checkpoint within 9hr:\ntotal_steps = 10  # adjust to your training plan\nlora_params = params  # replace with updated params during training\n\nfor step in range(1, total_steps + 1):\n    # ... perform training step here ...\n    # lora_params = updated params from training\n    if step % 5 == 0 or step == total_steps:\n        save_actor_checkpoint(step, lora_params)\n\nprint(\"Training complete. Last checkpoint saved at step:\", total_steps)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:16:14.463837Z","iopub.execute_input":"2025-12-30T19:16:14.464260Z","iopub.status.idle":"2025-12-30T19:16:14.697417Z","shell.execute_reply.started":"2025-12-30T19:16:14.464229Z","shell.execute_reply":"2025-12-30T19:16:14.696427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load latest checkpoint\nactor_ckpt_dir = os.path.join(CKPT_DIR, \"actor\")\nlatest_step = -1\nif os.path.exists(actor_ckpt_dir):\n    for item in os.listdir(actor_ckpt_dir):\n        if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r\"^\\d+$\", item):\n            step = int(item)\n            if step > latest_step:\n                latest_step = step\n\nif latest_step == -1:\n    raise FileNotFoundError(f\"No checkpoints found in {actor_ckpt_dir}\")\n\nprint(f\"Latest checkpoint step: {latest_step}\")\n\nwandb.init(project='tunix-eval', mode=\"disabled\")  # logging bug workaround\n\ntrained_ckpt_path = os.path.join(CKPT_DIR, \"actor\", str(latest_step), \"model_params\")\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    params\n)\ntrained_lora_params = ocp.StandardCheckpointer().restore(trained_ckpt_path, target=abs_params)\n\n# Update policy with trained LoRA params\ndef tree_update(tree, new_tree):\n    return jax.tree.map(lambda a, b: b, tree, new_tree)\n\nparams = tree_update(params, trained_lora_params)\n\n# Simple sampler wrapper (placeholder)\nclass Sampler:\n    def __init__(self, transformer, tokenizer):\n        self.transformer = transformer\n        self.tokenizer = tokenizer\n    def generate(self, question):\n        prompt = PROMPT_TEMPLATE.format(question=question)\n        # Use base_model for text generation; LoRA would apply in a full Flax transformer setup\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n        outputs = base_model.generate(\n            **inputs,\n            max_new_tokens=128,\n            temperature=TEMPERATURE,\n            top_k=TOP_K,\n            top_p=TOP_P,\n            do_sample=(TEMPERATURE > 0.0)\n        )\n        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nsampler = Sampler(lora_policy, tokenizer)\n\n# AI-based evaluation scaffold\nclass TunixHackathonEval:\n    questions = [\"What is LoRA?\", \"Explain Bayesian priors.\", \"Trade-offs in RLHF.\"]\n    ai_judge = \"ai\"\n\n    def __init__(self, sampler, prompt_template, temperature, top_k, top_p, seed):\n        self.sampler = sampler\n        self.template = prompt_template\n        self.temperature = temperature\n        self.top_k = top_k\n        self.top_p = top_p\n        self.seed = seed\n\n    def evaluate(self):\n        results = []\n        for q in self.questions:\n            ans = self.sampler.generate(q)\n            results.append({\"question\": q, \"answer\": ans})\n        return results\n\nPROMPT = PROMPT_TEMPLATE\nResult = TunixHackathonEval(sampler, PROMPT, TEMPERATURE, TOP_K, TOP_P, SEED).evaluate()\n\nprint(REASONING_START + \"eval_complete\" + REASONING_END)\nprint(SOLUTION_START + str(Result) + SOLUTION_END)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:16:17.286790Z","iopub.execute_input":"2025-12-30T19:16:17.287156Z","iopub.status.idle":"2025-12-30T19:19:12.583507Z","shell.execute_reply.started":"2025-12-30T19:16:17.287127Z","shell.execute_reply":"2025-12-30T19:19:12.582526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Publish Flax-format files to Kaggle Models (unrestricted mode)\n# Ensure you have converted/packaged your Flax parameters and configs accordingly.\n\nimport kagglehub\n\n# Example model path layout:\n# /kaggle/working/unrestricted/jax/size/actor/<latest_step>/model_params\nunrestricted_root = \"/kaggle/working/unrestricted/jax/size\"\nos.makedirs(unrestricted_root, exist_ok=True)\n\n# Copy/prepare your final checkpoint to unrestricted_root as required by Tunix\n# (You may need to mirror the directory structure used in evaluation.)\nfinal_src = trained_ckpt_path  # from above\nfinal_dst = os.path.join(unrestricted_root, \"actor\", str(latest_step), \"model_params\")\nos.makedirs(os.path.dirname(final_dst), exist_ok=True)\n\n# Orbax checkpoints are directories; ensure proper copying if needed.\n# For simplicity, re-save into final_dst\nocp.StandardCheckpointer().save(final_dst, trained_lora_params, force=True)\n\n# Upload to Kaggle Models\n# Replace 'your_username/model_name' and ensure Visibility set to Public in model settings.\n# Note: In practice, you may need to use Kaggle web UI to publish as a Model with JAX/Flax files.\n# kagglehub.model_upload(model_id=\"your_username/model_name\", src_dir=\"/kaggle/working/unrestricted\")\n\n# Record your unrestricted model ID here (make public and ensure loadable):\nunrestricted_kaggle_model = \"vijayarajan/gemma2-lora-reasoning-v1\"\nprint(\"Unrestricted mode model ID:\", unrestricted_kaggle_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:19:12.710291Z","iopub.execute_input":"2025-12-30T19:19:12.710573Z","iopub.status.idle":"2025-12-30T19:19:12.734072Z","shell.execute_reply.started":"2025-12-30T19:19:12.710551Z","shell.execute_reply":"2025-12-30T19:19:12.731930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CKPT_DIR = \"/kaggle/working/ckpts\"\nactor_dir = os.path.join(CKPT_DIR, \"actor\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:19:12.747015Z","iopub.execute_input":"2025-12-30T19:19:12.747329Z","iopub.status.idle":"2025-12-30T19:19:12.757998Z","shell.execute_reply.started":"2025-12-30T19:19:12.747302Z","shell.execute_reply":"2025-12-30T19:19:12.756580Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"step_dir = os.path.join(actor_dir, str(step))\nsave_path = os.path.join(step_dir, \"model_params\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:19:12.808982Z","iopub.execute_input":"2025-12-30T19:19:12.809572Z","iopub.status.idle":"2025-12-30T19:19:12.819375Z","shell.execute_reply.started":"2025-12-30T19:19:12.809538Z","shell.execute_reply":"2025-12-30T19:19:12.818292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpointer.save(save_path, lora_params, force=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:19:12.820395Z","iopub.execute_input":"2025-12-30T19:19:12.820776Z","iopub.status.idle":"2025-12-30T19:19:12.874684Z","shell.execute_reply.started":"2025-12-30T19:19:12.820743Z","shell.execute_reply":"2025-12-30T19:19:12.867369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"options = ocp.CheckpointManagerOptions(async_options=ocp.AsyncOptions())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:19:12.876380Z","iopub.execute_input":"2025-12-30T19:19:12.876769Z","iopub.status.idle":"2025-12-30T19:19:12.888270Z","shell.execute_reply.started":"2025-12-30T19:19:12.876745Z","shell.execute_reply":"2025-12-30T19:19:12.883209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_actor_checkpoint(step: int, lora_params):\n    step_dir = os.path.join(actor_dir, str(step))\n    os.makedirs(step_dir, exist_ok=True)\n\n    save_path = os.path.join(step_dir, \"model_params\")\n\n    # Synchronous save (Orbax returns None)\n    checkpointer.save(save_path, lora_params, force=True)\n\n    print(f\"Checkpoint saved at step {step} ‚Üí {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:19:12.892836Z","iopub.execute_input":"2025-12-30T19:19:12.893387Z","iopub.status.idle":"2025-12-30T19:19:12.905492Z","shell.execute_reply.started":"2025-12-30T19:19:12.893333Z","shell.execute_reply":"2025-12-30T19:19:12.904188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load uploaded checkpoint for unrestricted eval\n# May require appending exact subpaths depending on your publication layout\n# Example (adjust if needed):\n# trained_ckpt_path = kagglehub.model_download(unrestricted_kaggle_model + \"/jax/size\")\n\n# If your structure includes actor/<step>/model_params:\n# trained_ckpt_path = kagglehub.model_download(unrestricted_kaggle_model + \"/jax/size/actor/\" + str(latest_step) + \"/model_params\")\n\n# Restore and update params (same as single-session)\nabs_params = jax.tree.map(lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype), params)\ntrained_lora_params = ocp.StandardCheckpointer().restore(trained_ckpt_path, target=abs_params)\nparams = tree_update(params, trained_lora_params)\n\n# Reuse TunixHackathonEval from above\nResult = TunixHackathonEval(sampler, PROMPT, TEMPERATURE, TOP_K, TOP_P, SEED).evaluate()\nprint(REASONING_START + \"unrestricted_eval_complete\" + REASONING_END)\nprint(SOLUTION_START + str(Result) + SOLUTION_END)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:12:59.660092Z","iopub.execute_input":"2025-12-30T19:12:59.660431Z","iopub.status.idle":"2025-12-30T19:16:04.000017Z","shell.execute_reply.started":"2025-12-30T19:12:59.660405Z","shell.execute_reply":"2025-12-30T19:16:03.998781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unrestricted_kaggle_model = \"vijayarajan/gemma-1_1-2b-it-lora-reasoning\"\nprint(\"Unrestricted mode model ID:\", unrestricted_kaggle_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:19:13.033473Z","iopub.execute_input":"2025-12-30T19:19:13.033890Z","iopub.status.idle":"2025-12-30T19:19:13.054470Z","shell.execute_reply.started":"2025-12-30T19:19:13.033841Z","shell.execute_reply":"2025-12-30T19:19:13.053519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LoRALinear(nn.Module):\n    features: int\n    rank: int = 8\n    alpha: float = 16.0\n\n    def setup(self):\n        self.lora_A = self.param(\"lora_A\", nn.initializers.normal(0.02), (self.rank, self.features))\n        self.lora_B = self.param(\"lora_B\", nn.initializers.normal(0.02), (self.features, self.rank))\n        self.scaling = self.alpha / self.rank\n\n    def __call__(self, x, base_out):\n        lora_out = x @ self.lora_A.T @ self.lora_B.T\n        return base_out + self.scaling * lora_out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:20:46.213312Z","iopub.execute_input":"2025-12-30T19:20:46.213685Z","iopub.status.idle":"2025-12-30T19:20:46.222178Z","shell.execute_reply.started":"2025-12-30T19:20:46.213660Z","shell.execute_reply":"2025-12-30T19:20:46.220923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_lora_to_attention(params, lora_params):\n    for layer_name in params[\"transformer\"][\"layers\"]:\n        attn = params[\"transformer\"][\"layers\"][layer_name][\"attention\"]\n\n        for proj in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]:\n            base_w = attn[proj][\"kernel\"]\n            lora_A = lora_params[layer_name][proj][\"lora_A\"]\n            lora_B = lora_params[layer_name][proj][\"lora_B\"]\n\n            attn[proj][\"kernel\"] = base_w + (lora_B @ lora_A) * (alpha / rank)\n\n    return params\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:20:50.312124Z","iopub.execute_input":"2025-12-30T19:20:50.312465Z","iopub.status.idle":"2025-12-30T19:20:50.318963Z","shell.execute_reply.started":"2025-12-30T19:20:50.312423Z","shell.execute_reply":"2025-12-30T19:20:50.317828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:58:48.009832Z","iopub.execute_input":"2025-12-30T18:58:48.010297Z","iopub.status.idle":"2025-12-30T18:59:05.910006Z","shell.execute_reply.started":"2025-12-30T18:58:48.010265Z","shell.execute_reply":"2025-12-30T18:59:05.908712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q optax\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:19:43.951093Z","iopub.execute_input":"2025-12-30T19:19:43.951432Z","iopub.status.idle":"2025-12-30T19:19:48.407813Z","shell.execute_reply.started":"2025-12-30T19:19:43.951407Z","shell.execute_reply":"2025-12-30T19:19:48.406641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optax\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:19:48.416604Z","iopub.execute_input":"2025-12-30T19:19:48.416905Z","iopub.status.idle":"2025-12-30T19:19:48.438245Z","shell.execute_reply.started":"2025-12-30T19:19:48.416878Z","shell.execute_reply":"2025-12-30T19:19:48.437271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tx = optax.adamw(learning_rate=1e-4)\nopt_state = tx.init(lora_params)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:19:51.455991Z","iopub.execute_input":"2025-12-30T19:19:51.456335Z","iopub.status.idle":"2025-12-30T19:19:51.464242Z","shell.execute_reply.started":"2025-12-30T19:19:51.456311Z","shell.execute_reply":"2025-12-30T19:19:51.463278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tx = optax.adamw(learning_rate=1e-4)\nopt_state = tx.init(lora_params)\n\ndef train_step(params, lora_params, opt_state, batch):\n    def loss_fn(lora_params):\n        logits = model.apply(apply_lora_to_attention(params, lora_params), batch[\"input_ids\"])\n        loss = cross_entropy(logits, batch[\"labels\"])\n        return loss\n\n    grads = jax.grad(loss_fn)(lora_params)\n    updates, opt_state = tx.update(grads, opt_state)\n    lora_params = optax.apply_updates(lora_params, updates)\n    return lora_params, opt_state\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:19:54.406629Z","iopub.execute_input":"2025-12-30T19:19:54.406993Z","iopub.status.idle":"2025-12-30T19:19:54.415510Z","shell.execute_reply.started":"2025-12-30T19:19:54.406967Z","shell.execute_reply":"2025-12-30T19:19:54.414577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nbase = \"/kaggle/working/unrestricted/jax/size/actor/10/model_params\"\n\nos.makedirs(base, exist_ok=True)\n\nprint(\"Directory structure created:\", base)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:19:57.863545Z","iopub.execute_input":"2025-12-30T19:19:57.864312Z","iopub.status.idle":"2025-12-30T19:19:57.870180Z","shell.execute_reply.started":"2025-12-30T19:19:57.864280Z","shell.execute_reply":"2025-12-30T19:19:57.869213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nbase = \"/kaggle/working/unrestricted/jax/size/actor/10/model_params\"\nos.makedirs(base, exist_ok=True)\n\nprint(\"Directory structure created:\", base)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:20:01.943847Z","iopub.execute_input":"2025-12-30T19:20:01.944167Z","iopub.status.idle":"2025-12-30T19:20:01.950368Z","shell.execute_reply.started":"2025-12-30T19:20:01.944145Z","shell.execute_reply":"2025-12-30T19:20:01.949200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unrestricted_root = \"/kaggle/working/unrestricted/jax/size\"\nactor_step_dir = f\"{unrestricted_root}/actor/{latest_step}/model_params\"\n\nos.makedirs(actor_step_dir, exist_ok=True)\n\nocp.StandardCheckpointer().save(\n    actor_step_dir,\n    trained_lora_params,\n    force=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:20:05.740909Z","iopub.execute_input":"2025-12-30T19:20:05.741270Z","iopub.status.idle":"2025-12-30T19:20:05.766833Z","shell.execute_reply.started":"2025-12-30T19:20:05.741243Z","shell.execute_reply":"2025-12-30T19:20:05.763327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nunrestricted_kaggle_model = \"vijayarajan/gemma2-lora-reasoning-v1\"\nprint(\"Unrestricted mode model ID:\", unrestricted_kaggle_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:20:09.592587Z","iopub.execute_input":"2025-12-30T19:20:09.592976Z","iopub.status.idle":"2025-12-30T19:20:09.599259Z","shell.execute_reply.started":"2025-12-30T19:20:09.592951Z","shell.execute_reply":"2025-12-30T19:20:09.597840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\nimport os\n\npath = kagglehub.model_download(\"vijayarajan/gemma2-lora-reasoning-v1\")\nprint(\"Downloaded to:\", path)\n\n# Now navigate inside\nmodel_params_path = os.path.join(path, \"jax\", \"size\", \"actor\", \"10\", \"model_params\")\nprint(\"Model params path:\", model_params_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:20:12.780271Z","iopub.execute_input":"2025-12-30T19:20:12.780642Z","iopub.status.idle":"2025-12-30T19:20:12.843965Z","shell.execute_reply.started":"2025-12-30T19:20:12.780617Z","shell.execute_reply":"2025-12-30T19:20:12.842138Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\nimport orbax.checkpoint as ocp\nimport jax\n\n# Download model\npath = kagglehub.model_download(\"vijayarajan/gemma2-lora-reasoning-v1/jax/size\")\n\n# Restore checkpoint\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    params  # base params structure\n)\n\ntrained_lora_params = ocp.StandardCheckpointer().restore(\n    f\"{path}/actor/{latest_step}/model_params\",\n    target=abs_params\n)\n\nprint(\"Model restored successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:20:32.205627Z","iopub.execute_input":"2025-12-30T19:20:32.205983Z","iopub.status.idle":"2025-12-30T19:20:32.348532Z","shell.execute_reply.started":"2025-12-30T19:20:32.205963Z","shell.execute_reply":"2025-12-30T19:20:32.347049Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null}]}